{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update these paths to match your directory structure\n",
    "ORIGINAL_DATASET_DIR = \"/workspace/dataset/ppe_coco_format\"\n",
    "AUGMENTED_DATASET_DIR = \"/workspace/dataset/ppe_coco_format_augmented\"\n",
    "\n",
    "ORIGINAL_ANNOTATION_FILE = os.path.join(ORIGINAL_DATASET_DIR, \"annotations\", \"instances_train.json\")\n",
    "ORIGINAL_IMAGE_DIR = os.path.join(ORIGINAL_DATASET_DIR, \"images\", \"train\")\n",
    "\n",
    "AUGMENTED_IMAGE_DIR = os.path.join(AUGMENTED_DATASET_DIR, \"images\", \"train\")\n",
    "AUGMENTED_ANNOTATION_FILE = os.path.join(AUGMENTED_DATASET_DIR, \"annotations\", \"instances_train.json\")\n",
    "\n",
    "# Number of times to augment each image (e.g., 1x, 2x, etc.)\n",
    "# A value of 2 means each original image will be used to generate 2 new augmented images.\n",
    "AUGMENTATION_FACTOR = 2\n",
    "\n",
    "# --- Define Augmentation Pipeline ---\n",
    "# This pipeline includes augmentations that affect lighting, color, and noise.\n",
    "# Bounding box parameters are set to ensure they are handled correctly.\n",
    "# We will apply this to all images for a more robust model.\n",
    "augmentations = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.5),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "    A.MotionBlur(p=0.5),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['class_ids']))\n",
    "\n",
    "def augment_dataset():\n",
    "    \"\"\"\n",
    "    Reads a COCO dataset, applies augmentations, and saves a new augmented dataset.\n",
    "    \"\"\"\n",
    "    print(\"Starting dataset augmentation process...\")\n",
    "\n",
    "    # Create necessary directories\n",
    "    os.makedirs(AUGMENTED_IMAGE_DIR, exist_ok=True)\n",
    "    os.makedirs(os.path.join(AUGMENTED_DATASET_DIR, \"annotations\"), exist_ok=True)\n",
    "\n",
    "    # Load original COCO annotations\n",
    "    with open(ORIGINAL_ANNOTATION_FILE, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Initialize new COCO data structure for the augmented dataset\n",
    "    new_coco_data = {\n",
    "        \"info\": coco_data[\"info\"],\n",
    "        \"licenses\": coco_data[\"licenses\"],\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": coco_data[\"categories\"]\n",
    "    }\n",
    "\n",
    "    image_id_counter = 0\n",
    "    annotation_id_counter = 0\n",
    "\n",
    "    # Process original images and annotations\n",
    "    for image_info in coco_data[\"images\"]:\n",
    "        image_path = os.path.join(ORIGINAL_IMAGE_DIR, image_info[\"file_name\"])\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Get annotations for the current image\n",
    "        image_annotations = [ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] == image_info[\"id\"]]\n",
    "\n",
    "        # Prepare annotations for Albumentations (COCO format)\n",
    "        bboxes = [ann[\"bbox\"] for ann in image_annotations]\n",
    "        class_ids = [ann[\"category_id\"] for ann in image_annotations]\n",
    "\n",
    "        # Add the original image and annotations to the new dataset\n",
    "        new_coco_data[\"images\"].append({\n",
    "            \"id\": image_id_counter,\n",
    "            \"file_name\": image_info[\"file_name\"],\n",
    "            \"width\": image_info[\"width\"],\n",
    "            \"height\": image_info[\"height\"]\n",
    "        })\n",
    "        for ann in image_annotations:\n",
    "            ann['id'] = annotation_id_counter\n",
    "            ann['image_id'] = image_id_counter\n",
    "            new_coco_data[\"annotations\"].append(ann)\n",
    "            annotation_id_counter += 1\n",
    "        image_id_counter += 1\n",
    "\n",
    "        # Apply augmentations AUGMENTATION_FACTOR times\n",
    "        for i in range(AUGMENTATION_FACTOR):\n",
    "            try:\n",
    "                # Apply the augmentation pipeline\n",
    "                augmented = augmentations(\n",
    "                    image=image,\n",
    "                    bboxes=bboxes,\n",
    "                    class_ids=class_ids\n",
    "                )\n",
    "\n",
    "                # Save the new augmented image\n",
    "                new_file_name = f\"{os.path.splitext(image_info['file_name'])[0]}_aug{i}.jpg\"\n",
    "                new_image_path = os.path.join(AUGMENTED_IMAGE_DIR, new_file_name)\n",
    "                cv2.imwrite(new_image_path, augmented['image'])\n",
    "\n",
    "                # Save the new image info\n",
    "                new_coco_data[\"images\"].append({\n",
    "                    \"id\": image_id_counter,\n",
    "                    \"file_name\": new_file_name,\n",
    "                    \"width\": image_info[\"width\"],\n",
    "                    \"height\": image_info[\"height\"]\n",
    "                })\n",
    "\n",
    "                # Add the new annotations to the data\n",
    "                for ann_bbox, ann_class in zip(augmented['bboxes'], augmented['class_ids']):\n",
    "                    new_coco_data[\"annotations\"].append({\n",
    "                        \"id\": annotation_id_counter,\n",
    "                        \"image_id\": image_id_counter,\n",
    "                        \"category_id\": ann_class,\n",
    "                        \"bbox\": ann_bbox,\n",
    "                        \"area\": ann_bbox[2] * ann_bbox[3], # w * h\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    annotation_id_counter += 1\n",
    "\n",
    "                image_id_counter += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error augmenting image {image_info['file_name']}: {e}\")\n",
    "\n",
    "    # Save the new, combined COCO annotation file\n",
    "    with open(AUGMENTED_ANNOTATION_FILE, 'w') as f:\n",
    "        json.dump(new_coco_data, f, indent=4)\n",
    "\n",
    "    print(\"Dataset augmentation complete.\")\n",
    "    print(f\"Original images: {len(coco_data['images'])}\")\n",
    "    print(f\"Augmented images: {len(new_coco_data['images']) - len(coco_data['images'])}\")\n",
    "    print(f\"Total images in new dataset: {len(new_coco_data['images'])}\")\n",
    "    print(f\"New annotation file saved to: {AUGMENTED_ANNOTATION_FILE}\")\n",
    "\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    augment_dataset()"
   ],
   "id": "517753d98e12037d"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from rfdetr import RFDETRBase\n",
    "model = RFDETRBase(\n",
    "    size=\"base\",\n",
    "    resolution=672,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "checkpoint_path = \"/workspace/rfdetr_output/checkpoint.pth\"\n",
    "\n",
    "history = model.train(\n",
    "    dataset_dir=\"/workspace/dataset/ppe_coco_format\",\n",
    "    output_dir=\"/workspace/rfdetr_output\",\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    grad_accum_steps=24,\n",
    "    lr=1e-4,\n",
    "    warmup_epochs=3,\n",
    "    lr_scheduler=\"cosine\",\n",
    "    weight_decay=1e-4,\n",
    "    optimizer=\"adamw\",\n",
    "    clip_grad_norm=1.0,\n",
    "    fp16=True,\n",
    "    num_workers=4,\n",
    "    checkpoint_interval=2,\n",
    "    save_best_model=True,\n",
    "    tensorboard=True,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=5,\n",
    "    resume=checkpoint_pathÂ \n",
    ")"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
